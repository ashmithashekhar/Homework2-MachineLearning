# Homework 2 â€” Machine Learning (CS5710)  
**University of Central Missouri â€” Fall 2025**  
**Course:** CS5710 Machine Learning  
**Student:** Ashmitha Kumbham  

---

## ğŸ“˜ Assignment Overview  
**Home Assignment 2**  

- **Part A:** Q1â€“Q6 (theory)  
- **Part B:** Q7â€“Q9 (programming)  

ğŸ“‚ **Repository:** Homework2ML  
_All code, PDFs, and explanations are included in this repository._  

---

## ğŸ“‘ Repository Structure  
- **Homework 2 ML(1â€“6).pdf** â†’ Assignment write-up and solutions for Part A (theory).  
- **homework2_7_to_9.py** â†’ Python script for Part B (programming).  
- **HOMEWORK2_7_to_9.ipynb** â†’ Jupyter/Colab notebook with code + plots.  
- **README.md** â†’ This file (full explanations + run instructions).  

---

## ğŸ”§ How to Run the Code (Part B)  

### Option 1: Google Colab  
1. Open **Google Colab**.  
2. Upload `homework2_7_to_9.py`.  
3. Run all cells (self-contained).  

### Option 2: Local Python Environment  
```bash
# Check Python version
python -V   # Python 3.9+ recommended

# Install required packages
pip install numpy pandas scikit-learn matplotlib

# Run the script
python homework2_7_to_9.py
ğŸ“˜ Part A â€” Theory (Q1â€“Q6)
Q1. Decision Stump Prediction
Rule: predict â€œ+â€ if Sneezing=Yes, else â€œâˆ’â€.

Data: (Yes,+), (No,âˆ’), (Yes,âˆ’), (No,âˆ’).

Predictions: +, âˆ’, +, âˆ’ â†’ 1 mistake out of 4 (25% error).

Memorizer (perfect recall) â†’ 0% error (but poor generalization).

Q2. Training Error as Splitting Criterion
Dataset: 6 rows with Age, Exercise, Diet features.

Training errors at root split:

Age â†’ 16.7%

Exercise â†’ 16.7%

Diet â†’ 16.7%

âœ… Best split: tie across all three.

Q3. Entropy & Information Gain
Labels: Yes=3, No=3 â†’ Entropy = 1.0

Split on Exercise â†’ weighted entropy = 0.333

Information Gain = 0.667 â†’ good split

Q4. Confusion Matrix Metrics
Confusion matrix: TP=25, FN=5, FP=15, TN=55

Metrics:

Accuracy = 80%

Precision = 62.5%

Recall = 83.3%

Specificity = 78.6%

F1 â‰ˆ 0.714

âš ï¸ Imbalanced case â†’ Accuracy is misleading; F1/PR are better.

Q5. kNN Distance Calculations
Points: A(2,4,Red), B(4,4,Blue), C(4,6,Red); P(5,4)

Distances:

d(P,A) = 3

d(P,B) = 1

d(P,C) = 2.24

Results:

1-NN â†’ Blue

3-NN â†’ Red

Q6. 4-Fold Cross Validation
Errors:

k=1 â†’ 0.225

k=3 â†’ 0.1625

k=5 â†’ 0.1375

âœ… Best generalization: k=5

ğŸ’» Part B â€” Programming (Q7â€“Q9)
Q7. Decision Trees on Iris Dataset
Trained with max_depth = 1, 2, 3

Observations:

Depth=1 â†’ underfit, low accuracy.

Depth=2/3 â†’ better accuracy, good fit.

Too deep â†’ overfitting risk.

Q8. kNN with (sepal length, sepal width)
Trained with k=1, 3, 5, 10

Observations:

k=1 â†’ jagged decision boundaries (high variance).

k=3/5 â†’ smoother, better trade-off.

k=10 â†’ too smooth, underfit.

Q9. Performance Evaluation (k=5)
Confusion matrix + classification report.

ROC Curves (OvR + micro-average).

Results:

Accuracy > 90%

ROC/AUC â‰ˆ 1.0 â†’ strong separability

Micro-AUC confirms good overall performance.

ğŸ“Œ Academic Integrity
Part A: Step-by-step derivations from provided data.

Part B: Reproducible Python code with fixed random states.

âœï¸ Author
Ashmitha Kumbham
ğŸ“š Masterâ€™s in Data Science and AI
University of Central Missouri

