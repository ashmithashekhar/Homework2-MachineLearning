Homework 2 â€” Machine Learning (CS5710)

University of Central Missouri â€” Fall 2025
Course: CS5710 Machine Learning
Student: Ashmitha Kumbham

ğŸ“˜ Assignment Overview

This repository contains Homework Assignment 2 for Machine Learning:

Part A (Q1â€“Q6): Theory questions

Part B (Q7â€“Q9): Programming tasks

All code, PDFs, and explanations are included here.

ğŸ“‚ Repository Structure

Homework 2 ML(1â€“6).pdf â†’ Assignment write-up and solutions for Part A (theory)

homework2_7_to_9.py â†’ Python script for Part B (programming)

HOMEWORK2_7_to_9.ipynb â†’ Jupyter/Colab notebook with code + plots

README.md â†’ Full explanations + run instructions

ğŸ”§ How to Run the Code (Part B)
Option 1: Google Colab

Open Google Colab

Upload homework2_7_to_9.py

Run all cells (self-contained)

Option 2: Local Python Environment
# Check Python version
python -V   # Python 3.9+ recommended

# Install required packages
pip install numpy pandas scikit-learn matplotlib

# Run the script
python homework2_7_to_9.py

ğŸ“˜ Part A â€” Theory (Q1â€“Q6)
Q1. Decision Stump Prediction

Rule: predict â€œ+â€ if Sneezing=Yes, else â€œâˆ’â€

Data: (Yes,+), (No,âˆ’), (Yes,âˆ’), (No,âˆ’)

Predictions: +, âˆ’, +, âˆ’ â†’ 25% error

Memorizer â†’ 0% error (but poor generalization)

Q2. Training Error as Splitting Criterion

Dataset: 6 rows with Age, Exercise, Diet

Errors: Age = 16.7%, Exercise = 16.7%, Diet = 16.7%

âœ… Best split: tie across all three

Q3. Entropy & Information Gain

Labels: Yes=3, No=3 â†’ Entropy = 1.0

Split on Exercise â†’ Weighted Entropy = 0.333

Information Gain = 0.667 â†’ strong split

Q4. Confusion Matrix Metrics

Confusion matrix: TP=25, FN=5, FP=15, TN=55

Metric	Value
Accuracy	80%
Precision	62.5%
Recall	83.3%
Specificity	78.6%
F1-Score	â‰ˆ 0.714

âš ï¸ Accuracy is misleading (imbalanced case) â†’ F1/PR are better.

Q5. kNN Distance Calculations

Points: A(2,4,Red), B(4,4,Blue), C(4,6,Red); P(5,4)

d(P,A) = 3

d(P,B) = 1

d(P,C) = 2.24

Results:

1-NN â†’ Blue

3-NN â†’ Red

Q6. 4-Fold Cross Validation

Errors:

k=1 â†’ 0.225

k=3 â†’ 0.1625

k=5 â†’ 0.1375

âœ… Best generalization: k=5

ğŸ’» Part B â€” Programming (Q7â€“Q9)
Q7. Decision Trees on Iris Dataset

max_depth = 1, 2, 3

Depth=1 â†’ underfit

Depth=2/3 â†’ better accuracy, good fit

Too deep â†’ risk of overfitting

Q8. kNN with (sepal length, sepal width)

Trained with k=1, 3, 5, 10

k=1 â†’ jagged boundaries (high variance)

k=3/5 â†’ smoother, balanced

k=10 â†’ too smooth (underfit)

Q9. Performance Evaluation (k=5)

Confusion matrix + classification report

ROC curves (OvR + micro-average)

Results:

Accuracy > 90%

ROC/AUC â‰ˆ 1.0 â†’ strong separability

Micro-AUC confirms good performance

ğŸ“Œ Academic Integrity

Part A: Step-by-step derivations from provided data

Part B: Reproducible Python code with fixed random states

âœï¸ Author

Ashmitha Kumbham
ğŸ“š Masterâ€™s in Data Science and AI
University of Central Missouri

