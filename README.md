Homework 2 â€” Machine Learning (CS5710)

University of Central Missouri â€” Fall 2025

Student: Ashmitha Kumbham
Course: CS5710 Machine Learning
Assignment: Home Assignment 2 â€” Part A (Q1â€“Q6, theory) & Part B (Q7â€“Q9, programming)

ğŸ“‚ GitHub Repository: Homework2ML

(All code, PDFs, and explanations are available here.)

Repo Structure

Homework 2 ML(1â€“6).pdf â€” Assignment write-up and solutions for Part A (theory).

homework2_7_to_9.py â€” Python script for Part B (programming).

HOMEWORK2_7_to_9.ipynb â€” Jupyter/Colab notebook with code + plots.

README.md â€” this file (full explanations + run instructions).

ğŸ”§ How to Run the Code (Part B)
Option 1: Google Colab

Open Google Colab.

Upload homework2_7_to_9.py.

Run all cells (self-contained).

Option 2: Local Python
python -V            # Python 3.9+ recommended
pip install numpy pandas scikit-learn matplotlib
python homework2_7_to_9.py

Part A â€” Theory (Q1â€“Q6)
Q1. Decision Stump Prediction

Rule: predict â€œ+â€ if Sneezing=Yes, else â€œâˆ’â€.

Data: (Yes,+), (No,âˆ’), (Yes,âˆ’), (No,âˆ’).

Predictions: +, âˆ’, +, âˆ’ â†’ 1 mistake out of 4.

Training error = 25%.

Memorizer (perfect recall) = 0% error (but poor generalization).

Q2. Training Error as Splitting Criterion

6-row dataset with Age, Exercise, Diet features.

Compute training error for root split:

Age â†’ 16.7%

Exercise â†’ 16.7%

Diet â†’ 16.7%

Best split: tie between all three.

Q3. Entropy & Information Gain

Labels: Yes=3, No=3 â†’ Entropy = 1.0.

After splitting on Exercise: weighted entropy = 0.333.

Information Gain = 0.667 â†’ good split.

Q4. Confusion Matrix Metrics

Confusion matrix (TP=25, FN=5, FP=15, TN=55).

Accuracy = 80%, Precision = 62.5%, Recall = 83.3%, Specificity = 78.6%, F1 â‰ˆ 0.714.

Imbalanced case: accuracy misleads; prefer F1/PR metrics.

Q5. kNN Distance Calculations

Points: A(2,4,Red), B(4,4,Blue), C(4,6,Red); P(5,4).

Distances: d(P,A)=3, d(P,B)=1, d(P,C)=2.24.

1-NN â†’ Blue; 3-NN â†’ Red.

Q6. 4-Fold Cross Validation

Errors:

k=1 â†’ 0.225

k=3 â†’ 0.1625

k=5 â†’ 0.1375

Best generalization: k=5.

Part B â€” Programming (Q7â€“Q9)
Q7. Decision Trees on Iris

Train trees with max_depth = 1, 2, 3.

Observations:

Depth=1 â†’ underfit, lower accuracy.

Depth=2/3 â†’ better fit, higher accuracy.

Too deep â†’ risk of overfitting.

Q8. kNN (2 features: sepal length, sepal width)

Train kNN with k=1,3,5,10.

Plots: decision boundaries for each k.

Observations:

k=1 â†’ jagged, high variance.

k=3/5 â†’ smoother, better balance.

k=10 â†’ too smooth, may underfit.

Q9. Performance Evaluation (kNN, k=5)

Compute confusion matrix + classification report.

ROC curves (OvR + micro-average).

Results:

High accuracy (>90%).

ROC/AUC close to 1.0 (well-separated classes).

Micro-AUC shows strong overall separability.

ğŸ“Œ Academic Integrity

Part A: step-by-step derivations based on provided data.

Part B: reproducible Python code with fixed random state.

âœï¸ Author: Ashmitha Kumbham
ğŸ“š Masterâ€™s in Data Science and AI, University of central Missouri
